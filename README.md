# Bharath Raj B A - Data Engineering Portfolio

*Exploring Data Realms: Bridging software development and data engineering to create scalable, real-world data solutions.*

---

## About Me

**Based:** Bengaluru, India  
**Current Role:** Jr. Data Engineer (Ex-Software Developer)  
**Email:** bharathba.raj@gmail.com  
**LinkedIn:** [My Profile](https://www.linkedin.com/in/bharath-raj-b-a-b46022145/)  
**GitHub:** [bharath-ba](https://github.com/bharath-ba)

Hi, I'm Bharath Raj — a data engineering enthusiast with a background in Information Science and Engineering. After a deep dive into software development, I transitioned into the world of data where I now specialize in building robust ETL pipelines, scalable data lakes, and efficient warehousing systems using Python, Azure, Databricks, and PySpark.

This portfolio showcases selected projects that demonstrate my hands-on skills, architectural thinking, and ability to work with real-world data engineering problems.

---

## Why Data Engineering?

Initially inclined toward software development and web technologies, I discovered a stronger alignment with data systems — particularly with designing how data flows, scales, and serves analytics. My analytical mindset, combined with programming fundamentals, made data engineering the ideal domain where I could combine logic, performance, and structure.

---

## Education

- **B.E in Information Science & Engineering**  
  Dayananda Sagar College of Engineering, Bengaluru  
  *Sept 2018 – Sept 2021 | GPA: 7.5/10*

---

## Certifications

- [Databricks Certified Data Engineer Associate](https://credentials.databricks.com/cfc95251-55e5-4c16-851c-c6511825aa77#acc.jUpJAv2S)  
- [Introduction to Data Engineering – Coursera](https://coursera.org/share/789fbbbe6b608dafd1f03fe6936c282f)  
- [Award for Project Delivery Excellence](https://drive.google.com/file/d/1Rik6_Ramp7vc0g6t87ZB15zuXGS3zHYz/view?usp=sharing)

---

## Portfolio Projects (7 Selected)

### 1. **Retail Data Lakehouse Pipeline**  
**Tech Stack:** Azure ADF, Databricks, PySpark, Delta Lake, Synapse  
**Code:** [GitHub Repo](https://github.com/bharath-ba/Retail_Data_Lakehouse)  

Built an end-to-end pipeline using a Medallion (Bronze-Silver-Gold) architecture. Implemented batch ingestion with ADF, transformations using PySpark, and dimensional modeling with SCD Type 2 logic. Output data was made analytics-ready via Synapse and connected to Power BI.

---

### 2. **Streaming Clickstream Analytics** *(Ongoing)*  
**Tech Stack:** Kafka, Spark Streaming, PostgreSQL, Azure  

Simulates real-time clickstream events and processes them using Spark Structured Streaming. Aggregated results are stored in PostgreSQL and visualized through dashboards. Focus on low-latency ingestion, checkpointing, and streaming window functions.

---

### 3. **Automated Data Reporting Workflow**  
**Tech Stack:** Python, Azure Functions, ADF  

Developed a fully automated reporting tool to extract, clean, and email daily reports to business stakeholders. Reduced manual effort by 28% using Python-based scheduling and Azure automation.

---

### 4. **Power Grid Data Pipeline (IEEE-118 Simulation)**  
**Tech Stack:** PySpark, Pandapower, Delta Lake, Azure Data Lake  

Ingested NREL IEEE-118 case data and simulated hourly power flows. Stored results using Delta format for auditability. Deployed in ADF and visualized performance metrics (e.g., bus load, outages).

---

### 5. **Data Lineage Tracker for ADF Pipelines**  
**Tech Stack:** Azure Data Factory, Python, Databricks  

Designed a metadata capture layer that logs pipeline execution context (source, transformations, target, status) to a lineage DB. Useful for audits and debugging failed runs.

---

### 6. **GitHub Issue Tracker Analytics**  
**Tech Stack:** GitHub API, Python, Pandas, Power BI  

Extracted and analyzed GitHub issue/PR activity from a target repo to understand contributor patterns and issue resolutions. Processed data using Python and built dashboards in Power BI.

---

### 7. **Data Quality Validator Framework**  
**Tech Stack:** PySpark, Great Expectations, ADF  

Integrated Great Expectations into PySpark pipelines for automated validation. Built tests for null checks, uniqueness, data type compliance, and threshold ranges. Failures are logged and alerted via email/Teams.

---

Thanks for visiting my portfolio. Feel free to reach out if you're hiring for a data engineer or need someone who can bring order, efficiency, and scale to your data systems.

Sincerely,  
**Bharath Raj B A**
